//! CockroachDB è¿ç§»æ‰§è¡Œæ¨¡å—
//! æ‰‹åŠ¨æ‰§è¡Œè¿ç§»æ–‡ä»¶ï¼Œç»•è¿‡ advisory locks é™åˆ¶

use anyhow::{Context, Result};
use sqlx::PgPool;
use std::fs;
use std::path::{Path, PathBuf};

/// åˆ†å‰²SQLè¯­å¥ï¼ˆæŒ‰åˆ†å·åˆ†å‰²ï¼Œä½†ä¿ç•™å­—ç¬¦ä¸²å’Œæ³¨é‡Šçš„å®Œæ•´æ€§ï¼‰
fn split_sql_statements(sql: &str) -> Vec<String> {
    let mut statements = Vec::new();
    let mut current = String::new();
    let mut in_string = false;
    let mut string_char = None;
    let mut in_comment = false;
    let mut chars = sql.chars().peekable();

    while let Some(ch) = chars.next() {
        if in_comment {
            if ch == '\n' {
                in_comment = false;
                current.push(ch);
            } else {
                current.push(ch);
            }
            continue;
        }

        if !in_string && ch == '-' && chars.peek() == Some(&'-') {
            // å•è¡Œæ³¨é‡Šå¼€å§‹
            in_comment = true;
            current.push(ch);
            if let Some(next) = chars.next() {
                current.push(next);
            }
            continue;
        }

        if !in_string && (ch == '\'' || ch == '"') {
            // å­—ç¬¦ä¸²å¼€å§‹
            in_string = true;
            string_char = Some(ch);
            current.push(ch);
        } else if in_string && ch == string_char.unwrap() {
            // æ£€æŸ¥æ˜¯å¦æ˜¯è½¬ä¹‰çš„å¼•å·
            if chars.peek() == Some(&ch) {
                // è½¬ä¹‰çš„å¼•å·ï¼ˆ'' æˆ– ""ï¼‰
                current.push(ch);
                if let Some(next) = chars.next() {
                    current.push(next);
                }
            } else {
                // å­—ç¬¦ä¸²ç»“æŸ
                in_string = false;
                string_char = None;
                current.push(ch);
            }
        } else if !in_string && ch == ';' {
            // è¯­å¥ç»“æŸ
            current.push(ch);
            let trimmed = current.trim().to_string();
            if !trimmed.is_empty() {
                statements.push(trimmed);
            }
            current.clear();
        } else {
            current.push(ch);
        }
    }

    // æ·»åŠ æœ€åä¸€ä¸ªè¯­å¥ï¼ˆå¦‚æœæ²¡æœ‰ä»¥åˆ†å·ç»“å°¾ï¼‰
    let trimmed = current.trim().to_string();
    if !trimmed.is_empty() {
        statements.push(trimmed);
    }

    statements
}

/// æŸ¥æ‰¾è¿ç§»ç›®å½•
/// æŒ‰ä¼˜å…ˆçº§æŸ¥æ‰¾ï¼š./migrations -> ./IronCore/migrations -> ../migrations
fn find_migrations_dir() -> Result<PathBuf> {
    let candidates = vec![
        Path::new("./migrations"),
        Path::new("./IronCore/migrations"),
        Path::new("../migrations"),
        Path::new("migrations"),
    ];

    let searched_paths: Vec<String> = candidates
        .iter()
        .map(|p| p.to_string_lossy().to_string())
        .collect();

    for candidate in &candidates {
        if candidate.exists() && candidate.is_dir() {
            tracing::debug!("Found migrations directory: {:?}", candidate);
            return Ok(candidate.to_path_buf());
        }
    }

    anyhow::bail!(
        "Migrations directory not found. Searched: {:?}",
        searched_paths
    );
}

/// è·å–è¿ç§»çŠ¶æ€æ‘˜è¦
pub async fn get_migration_status(pool: &PgPool) -> Result<MigrationStatus> {
    // åˆå§‹åŒ–è¿ç§»è¡¨ï¼ˆå¦‚æœä¸å­˜åœ¨ï¼‰
    crate::infrastructure::migration::init_migration_table(pool).await?;

    // è·å–å·²åº”ç”¨çš„è¿ç§»
    let applied = crate::infrastructure::migration::get_applied_migrations(pool).await?;

    // è·å–æ‰€æœ‰è¿ç§»æ–‡ä»¶
    let migrations_dir = find_migrations_dir()?;
    let entries = fs::read_dir(&migrations_dir).context("Failed to read migrations directory")?;

    let mut total_files = 0;
    for entry in entries {
        let entry = entry.context("Failed to read directory entry")?;
        let path = entry.path();
        if let Some(ext) = path.extension() {
            if ext == "sql" {
                if let Some(file_name) = path.file_name() {
                    let file_name_str = file_name.to_string_lossy();
                    if let Some(version_str) = file_name_str.split('_').next() {
                        if version_str.parse::<i64>().is_ok() {
                            total_files += 1;
                        }
                    }
                }
            }
        }
    }

    Ok(MigrationStatus {
        total_migrations: total_files,
        applied_count: applied.len(),
        latest_version: applied.last().map(|m| m.version),
    })
}

/// è¿ç§»çŠ¶æ€ä¿¡æ¯
#[derive(Debug, Clone)]
pub struct MigrationStatus {
    pub total_migrations: usize,
    pub applied_count: usize,
    pub latest_version: Option<i64>,
}

/// æ‰‹åŠ¨æ‰§è¡Œè¿ç§»æ–‡ä»¶ï¼ˆç”¨äºCockroachDBï¼‰
/// ç»•è¿‡advisory locksé™åˆ¶ï¼Œç›´æ¥æ‰§è¡ŒSQLæ–‡ä»¶
pub async fn run_migrations_manual(pool: &PgPool) -> Result<()> {
    tracing::info!("ğŸš€ Starting database migration upgrade...");

    // æ˜¾ç¤ºè¿ç§»çŠ¶æ€
    match get_migration_status(pool).await {
        Ok(status) => {
            tracing::info!(
                "ğŸ“Š Migration status: {}/{} applied, latest version: {:?}",
                status.applied_count,
                status.total_migrations,
                status.latest_version
            );
        }
        Err(e) => {
            tracing::warn!("âš ï¸  Could not get migration status: {}", e);
        }
    }

    // åˆå§‹åŒ–è¿ç§»è¡¨
    crate::infrastructure::migration::init_migration_table(pool).await?;

    // è·å–è¿ç§»ç›®å½•ï¼ˆè‡ªåŠ¨æŸ¥æ‰¾ï¼‰
    let migrations_dir = find_migrations_dir()?;
    tracing::info!("ğŸ“ Using migrations directory: {:?}", migrations_dir);

    // è¯»å–æ‰€æœ‰è¿ç§»æ–‡ä»¶
    let mut migration_files: Vec<(i64, String, String)> = Vec::new();
    let entries = fs::read_dir(&migrations_dir).context("Failed to read migrations directory")?;

    for entry in entries {
        let entry = entry.context("Failed to read directory entry")?;
        let path = entry.path();
        if let Some(ext) = path.extension() {
            if ext == "sql" {
                if let Some(file_name) = path.file_name() {
                    let file_name_str = file_name.to_string_lossy();
                    // è§£ææ–‡ä»¶åæ ¼å¼: 0001_name.sql
                    if let Some(version_str) = file_name_str.split('_').next() {
                        if let Ok(version) = version_str.parse::<i64>() {
                            let content = fs::read_to_string(&path)
                                .context(format!("Failed to read migration file: {:?}", path))?;
                            let name = file_name_str
                                .strip_suffix(".sql")
                                .unwrap_or(&file_name_str)
                                .to_string();
                            migration_files.push((version, name, content));
                        }
                    }
                }
            }
        }
    }

    // æŒ‰ç‰ˆæœ¬æ’åº
    migration_files.sort_by_key(|(v, _, _)| *v);

    if migration_files.is_empty() {
        tracing::warn!("âš ï¸  No migration files found in {:?}", migrations_dir);
        return Ok(());
    }

    tracing::info!("ğŸ“‹ Found {} migration file(s)", migration_files.len());

    // æ‰§è¡Œæ¯ä¸ªè¿ç§»
    let mut applied_count = 0;
    let mut skipped_count = 0;
    for (version, name, sql) in migration_files {
        // æ£€æŸ¥æ˜¯å¦å·²åº”ç”¨
        let is_applied = crate::infrastructure::migration::is_migration_applied(pool, version)
            .await
            .context("Failed to check migration status")?;

        if is_applied {
            tracing::debug!(
                "â­ï¸  Migration {} ({}) already applied, skipping",
                version,
                name
            );
            skipped_count += 1;
            continue;
        }

        tracing::info!("ğŸ”„ Applying migration {}: {}", version, name);

        // åˆ†å‰²SQLè¯­å¥ï¼ˆæŒ‰åˆ†å·åˆ†å‰²ï¼Œä½†å¿½ç•¥å­—ç¬¦ä¸²å’Œæ³¨é‡Šä¸­çš„åˆ†å·ï¼‰
        let statements = split_sql_statements(&sql);

        if statements.is_empty() {
            tracing::warn!(
                "âš ï¸  No SQL statements found in migration {}: {}",
                version,
                name
            );
            continue;
        }

        tracing::debug!(
            "  Found {} SQL statement(s) in migration {}",
            statements.len(),
            version
        );

        // ä¸ºæ¯ä¸ªè¯­å¥ä½¿ç”¨ç‹¬ç«‹çš„äº‹åŠ¡ï¼Œé¿å…"çº¦æŸå·²å­˜åœ¨"é”™è¯¯å¯¼è‡´æ•´ä¸ªäº‹åŠ¡å¤±è´¥
        for (idx, statement) in statements.iter().enumerate() {
            let statement = statement.trim();
            // è·³è¿‡ç©ºè¯­å¥å’Œæ³¨é‡Š
            if statement.is_empty() || statement.starts_with("--") {
                continue;
            }

            // æ˜¾ç¤ºæ›´è¯¦ç»†çš„æ‰§è¡Œä¿¡æ¯
            let statement_preview = if statement.len() > 80 {
                format!("{}...", &statement[..80])
            } else {
                statement.to_string()
            };

            tracing::debug!(
                "Executing statement {}/{} of migration {}: {}",
                idx + 1,
                statements.len(),
                version,
                statement_preview
            );

            // ä¸ºæ¯ä¸ªè¯­å¥ä½¿ç”¨ç‹¬ç«‹çš„äº‹åŠ¡
            let mut tx = pool.begin().await.context(format!(
                "Failed to start transaction for statement {} of migration {}: {}",
                idx + 1,
                version,
                name
            ))?;

            // æ‰§è¡ŒSQLè¯­å¥ï¼Œæ•è·è¯¦ç»†é”™è¯¯
            match sqlx::query(statement).execute(&mut *tx).await {
                Ok(_) => {
                    tracing::debug!("  âœ“ Statement {} executed successfully", idx + 1);
                    // æäº¤äº‹åŠ¡
                    tx.commit().await.context(format!(
                        "Failed to commit statement {} of migration {}: {}",
                        idx + 1,
                        version,
                        name
                    ))?;
                }
                Err(e) => {
                    let error_str = e.to_string();
                    // æ£€æŸ¥æ˜¯å¦æ˜¯"çº¦æŸå·²å­˜åœ¨"æˆ–"IF NOT EXISTS"ç›¸å…³çš„é”™è¯¯
                    // è¿™äº›é”™è¯¯å¯ä»¥å®‰å…¨å¿½ç•¥ï¼Œå› ä¸ºçº¦æŸ/ç´¢å¼•å·²ç»å­˜åœ¨
                    if error_str.contains("already exists")
                        || error_str.contains("duplicate key")
                        || error_str.contains("duplicate constraint name")
                        || (error_str.contains("constraint")
                            && error_str.contains("already exists"))
                        || (error_str.contains("index") && error_str.contains("already exists"))
                    {
                        // å›æ»šå½“å‰äº‹åŠ¡ï¼ˆè™½ç„¶å¤±è´¥äº†ï¼Œä½†éœ€è¦æ¸…ç†ï¼‰
                        let _ = tx.rollback().await;

                        tracing::warn!(
                            "  âš ï¸  Statement {}: Constraint/index already exists, skipping: {}",
                            idx + 1,
                            if error_str.len() > 150 {
                                format!("{}...", &error_str[..150])
                            } else {
                                error_str.clone()
                            }
                        );
                        // ç»§ç»­æ‰§è¡Œä¸‹ä¸€ä¸ªè¯­å¥ï¼Œä¸è¿”å›é”™è¯¯
                        continue;
                    }

                    // æ£€æŸ¥æ˜¯å¦æ˜¯ CockroachDB çš„ç‰¹æ®Šé”™è¯¯ï¼šéœ€è¦ä½¿ç”¨ DROP INDEX ä»£æ›¿ DROP CONSTRAINT
                    if error_str.contains("cannot drop UNIQUE constraint")
                        && error_str.contains("use DROP INDEX CASCADE")
                    {
                        // å›æ»šå½“å‰äº‹åŠ¡
                        let _ = tx.rollback().await;

                        tracing::warn!(
                            "  âš ï¸  Statement {}: CockroachDB requires DROP INDEX for UNIQUE constraints, skipping: {}",
                            idx + 1,
                            if error_str.len() > 150 {
                                format!("{}...", &error_str[..150])
                            } else {
                                error_str.clone()
                            }
                        );
                        // ç»§ç»­æ‰§è¡Œä¸‹ä¸€ä¸ªè¯­å¥ï¼Œä¸è¿”å›é”™è¯¯
                        continue;
                    }

                    // å›æ»šäº‹åŠ¡
                    let _ = tx.rollback().await;

                    let error_msg =
                        format!(
                        "Failed to execute statement {} of migration {}: {}\nSQL: {}\nError: {}",
                        idx + 1, version, name,
                        if statement.len() > 500 {
                            format!("{}...", &statement[..500])
                        } else {
                            statement.to_string()
                        },
                        e
                    );
                    tracing::error!("{}", error_msg);
                    return Err(anyhow::anyhow!(error_msg));
                }
            }
        }

        // è®°å½•è¿ç§»
        crate::infrastructure::migration::record_migration(pool, version, &name)
            .await
            .context(format!("Failed to record migration {}: {}", version, name))?;

        applied_count += 1;
        tracing::info!("âœ… Migration {} ({}) applied successfully", version, name);
    }

    // æ˜¾ç¤ºæœ€ç»ˆçŠ¶æ€
    tracing::info!("â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”");
    if applied_count > 0 {
        tracing::info!("âœ… Migration upgrade completed!");
        tracing::info!("   â€¢ Applied: {} migration(s)", applied_count);
        if skipped_count > 0 {
            tracing::info!(
                "   â€¢ Skipped: {} migration(s) (already applied)",
                skipped_count
            );
        }
    } else {
        tracing::info!("âœ… All migrations already applied");
        tracing::info!("   â€¢ Total: {} migration(s)", skipped_count);
    }
    tracing::info!("â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”");

    Ok(())
}

/// é‡ç½®æ•°æ®åº“ï¼šåˆ é™¤æ‰€æœ‰è¿ç§»è®°å½•ï¼Œå¼ºåˆ¶é‡æ–°è¿è¡Œæ‰€æœ‰è¿ç§»
/// âš ï¸ è­¦å‘Šï¼šè¿™ä¼šåˆ é™¤æ‰€æœ‰è¿ç§»è®°å½•ï¼Œä¸‹æ¬¡å¯åŠ¨æ—¶ä¼šé‡æ–°è¿è¡Œæ‰€æœ‰è¿ç§»
/// å¦‚æœè¡¨å·²å­˜åœ¨ï¼Œè¿ç§»å¯èƒ½ä¼šå¤±è´¥ï¼ˆå› ä¸ºè¡¨å·²å­˜åœ¨ï¼‰
pub async fn reset_migration_records(pool: &PgPool) -> Result<()> {
    tracing::warn!("âš ï¸  Resetting migration records - all migrations will be re-run on next start");

    sqlx::query("DELETE FROM schema_migrations")
        .execute(pool)
        .await
        .context("Failed to delete migration records")?;

    tracing::info!("âœ… Migration records cleared");
    Ok(())
}

/// å®Œå…¨é‡ç½®æ•°æ®åº“ï¼šåˆ é™¤æ‰€æœ‰è¡¨å’Œæ•°æ®
/// âš ï¸ è­¦å‘Šï¼šè¿™ä¼šåˆ é™¤æ‰€æœ‰æ•°æ®ï¼ä»…ç”¨äºå¼€å‘ç¯å¢ƒ
pub async fn drop_all_tables(pool: &PgPool) -> Result<()> {
    tracing::warn!("âš ï¸  DROPPING ALL TABLES - ALL DATA WILL BE LOST!");

    // è·å–æ‰€æœ‰è¡¨åï¼ˆåŒ…æ‹¬æ‰€æœ‰schemaï¼‰
    let tables: Vec<String> = sqlx::query_scalar(
        r#"
        SELECT table_schema || '.' || table_name as full_table_name
        FROM information_schema.tables
        WHERE table_type = 'BASE TABLE'
        AND table_schema NOT IN ('pg_catalog', 'information_schema', 'pg_extension', 'crdb_internal')
        ORDER BY table_schema, table_name
        "#
    )
    .fetch_all(pool)
    .await
    .context("Failed to query table names")?;

    if tables.is_empty() {
        tracing::info!("No tables to drop");
        return Ok(());
    }

    tracing::info!("Found {} tables to drop", tables.len());

    // åˆ é™¤æ‰€æœ‰è¡¨ï¼ˆCASCADEä¼šè‡ªåŠ¨å¤„ç†å¤–é”®ï¼‰
    let mut tx = pool.begin().await.context("Failed to start transaction")?;

    for table in &tables {
        tracing::info!("Dropping table: {}", table);
        sqlx::query(&format!("DROP TABLE IF EXISTS {} CASCADE", table))
            .execute(&mut *tx)
            .await
            .with_context(|| format!("Failed to drop table: {}", table))?;
    }

    tx.commit().await.context("Failed to commit transaction")?;

    tracing::info!("âœ… All tables dropped successfully");
    Ok(())
}

/// å®Œå…¨é‡ç½®æ•°æ®åº“å¹¶é‡æ–°è¿è¡Œè¿ç§»ï¼ˆå¼€å‘ç¯å¢ƒä¸“ç”¨ï¼‰
///
/// è¿™ä¼šï¼š
/// 1. åˆ é™¤æ‰€æœ‰è¡¨å’Œæ•°æ®
/// 2. åˆ é™¤è¿ç§»è®°å½•
/// 3. é‡æ–°è¿è¡Œæ‰€æœ‰è¿ç§»
///
/// âš ï¸ è­¦å‘Šï¼šè¿™ä¼šåˆ é™¤æ‰€æœ‰æ•°æ®ï¼ä»…ç”¨äºå¼€å‘ç¯å¢ƒ
pub async fn reset_database_clean(pool: &PgPool) -> Result<()> {
    tracing::warn!("â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”");
    tracing::warn!("âš ï¸  COMPLETE DATABASE RESET - ALL DATA WILL BE LOST!");
    tracing::warn!("âš ï¸  This is for DEVELOPMENT ONLY!");
    tracing::warn!("â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”");

    // æ­¥éª¤1: åˆ é™¤æ‰€æœ‰è¡¨
    tracing::info!("Step 1/3: Dropping all tables...");
    drop_all_tables(pool).await?;

    // æ­¥éª¤2: åˆ é™¤è¿ç§»è®°å½•ï¼ˆå¦‚æœè¡¨è¿˜å­˜åœ¨ï¼‰
    tracing::info!("Step 2/3: Clearing migration records...");
    // å…ˆå°è¯•åˆ é™¤è¿ç§»è¡¨ï¼ˆå¦‚æœè¿˜å­˜åœ¨ï¼‰
    let _ = sqlx::query("DROP TABLE IF EXISTS schema_migrations CASCADE")
        .execute(pool)
        .await;

    // æ­¥éª¤3: é‡æ–°è¿è¡Œæ‰€æœ‰è¿ç§»
    tracing::info!("Step 3/3: Running migrations on clean database...");
    run_migrations_manual(pool).await?;

    tracing::info!("â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”");
    tracing::info!("âœ… Database reset complete! Fresh database ready.");
    tracing::info!("â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”");

    Ok(())
}
